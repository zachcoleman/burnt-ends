{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-Pixel Convolution\n",
    "\n",
    "An implementation of the Sub-Pixel Convolution operation.\n",
    "\n",
    "Reference Links:\n",
    "- https://arxiv.org/pdf/1609.05158.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubPixelConv(torch.nn.Module):\n",
    "    def __init__(self, in_chans, r):\n",
    "        super().__init__()\n",
    "        self.in_chans = in_chans\n",
    "        self.r = r\n",
    "\n",
    "        # divide channels out\n",
    "        self.chans_per_subsection  = in_chans // r**2\n",
    "        ch_bounds = list(range(0, in_chans+1, self.chans_per_subsection))\n",
    "        self.channel_divisions = list(zip(ch_bounds[:-1], ch_bounds[1:]))\n",
    "\n",
    "        # build \"upsampling\" filters to expand that'll be used\n",
    "        # to expand single input features into a rxr space\n",
    "        filters = []\n",
    "        for i in range(r**2):\n",
    "            exp_filter = np.zeros((1, 1, r, r), np.float32)\n",
    "            exp_filter[..., i // r, i % r] = 1.\n",
    "            filters.append(exp_filter)\n",
    "        \n",
    "        self.expanding_transposed_convs = []\n",
    "        for f in filters:\n",
    "            exp_conv = torch.nn.ConvTranspose2d(1, 1, r, r, bias=False)\n",
    "            exp_conv.weight = torch.nn.Parameter(\n",
    "                torch.from_numpy(f),\n",
    "                requires_grad=False\n",
    "            )\n",
    "            self.expanding_transposed_convs.append(exp_conv)\n",
    "        self.expanding_transposed_convs = torch.nn.ModuleList(self.expanding_transposed_convs)\n",
    "\n",
    "    def forward_feature_map(self, inputs):\n",
    "        if inputs.shape[1] != self.in_chans:\n",
    "            raise ValueError(\"input channels does not expected `in_chans`\")\n",
    "\n",
    "        # for each subset of channels, use expanding tranposed conv\n",
    "        # and track all subset in list\n",
    "        by_channel_division = []\n",
    "        for exp, div in zip(self.expanding_transposed_convs, self.channel_divisions):\n",
    "            by_channel_division.append(\n",
    "                torch.cat([exp(inputs[:,[c],...]) for c in range(div[0], div[1])], dim=1)\n",
    "            )\n",
    "        \n",
    "        # stack the mutually exclusive expanded by-channel subsets\n",
    "        # then sum them up (this gathers the sub-divided up)\n",
    "        return torch.stack([t for t in by_channel_division]).sum(dim=0) \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if inputs.ndim == 3:\n",
    "            # could throw error here instead :shrug:\n",
    "            return self.forward_feature_map(inputs.unsqueeze(0))\n",
    "        elif inputs.ndim == 4:\n",
    "            return self.forward_feature_map(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 2\n",
    "in_chans = 100\n",
    "spc = SubPixelConv(in_chans, r)\n",
    "inputs = torch.ones(1, in_chans, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spc.to(DEVICE)\n",
    "inputs = inputs.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.39 ms ± 23.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = spc.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432 µs ± 31 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = torch.nn.functional.interpolate(\n",
    "    inputs,\n",
    "    size = (inputs.shape[-2]*r, inputs.shape[-1]*r),\n",
    "    mode = \"bilinear\",\n",
    "    align_corners=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c248e6104bd959e5724d79211e531f4a227ee51afecf81cca8a8f9654ab16a32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
