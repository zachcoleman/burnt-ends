{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Classifier\n",
    "\n",
    "An example of simple linear classifier (similar but not equivalent to a SVM). This however does **not** use support vectors, but instead uses a simple formulation and training over iteratively over batches.\n",
    "\n",
    "Reference Links: \n",
    "- https://en.wikipedia.org/wiki/Support-vector_machine\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.SoftMarginLoss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data gen\n",
    "X = np.random.normal(0, 1, (1_000, 2))\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# define our seperator (y = mx + b)\n",
    "m, b = 2, -1.2\n",
    "\n",
    "# do target classes\n",
    "class_bool = X[:, 1] < m*X[:, 0] + b + np.random.normal(0, 0.5, (1000,))\n",
    "y = -1.*class_bool + 1.*np.logical_not(class_bool)  # definition comes from SoftMarginLoss requirement\n",
    "y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data\n",
    "plt.scatter(X[class_bool, 0], X[class_bool, 1])\n",
    "plt.scatter(X[np.logical_not(class_bool), 0], X[np.logical_not(class_bool), 1])\n",
    "plt.plot(np.arange(-3, 3, 0.1), np.arange(-3, 3, 0.1)*m + b, linewidth=2, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = torch.nn.Parameter(torch.from_numpy(np.array([0.], dtype=np.float32)), requires_grad=True)\n",
    "        self.b = torch.nn.Parameter(torch.from_numpy(np.array([0.], dtype=np.float32)), requires_grad=True)\n",
    "    \n",
    "    def forward_train(self, X):\n",
    "        probs = torch.softmax(\n",
    "            torch.stack([X[:, 0] * self.m + self.b, X[:, 1]], dim=1),\n",
    "            dim=1\n",
    "        )\n",
    "        return -1.*probs[:,0] + 1.*probs[:,1]\n",
    "\n",
    "    def forward_test(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = torch.from_numpy(X)\n",
    "        class_bool = X[:, 1] < X[:, 0] * self.m + self.b\n",
    "        return -1.*class_bool + 1.*torch.logical_not(class_bool)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.m}, {self.b}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyRepeatDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._iter_X = iter(cycle(self.X))\n",
    "        self._iter_y = iter(cycle(self.y))\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self._iter_X), next(self._iter_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LinearClassifier()\n",
    "optim = torch.optim.SGD(lc.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.SoftMarginLoss()\n",
    "\n",
    "train_dataset = NumpyRepeatDataset(X, y)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 20,\n",
    ")\n",
    "iter_train_dataloader = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = 10_000\n",
    "loss_steps = set([i for i in range(total_steps//10, total_steps+1, total_steps//10)])\n",
    "viz_steps = set([1, 10, 50, 100, 200])\n",
    "viz_steps = set([])\n",
    "\n",
    "lc.to(DEVICE)\n",
    "\n",
    "import time\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for i in range(total_steps):\n",
    "    optim.zero_grad()\n",
    "    t_X, t_y = next(iter_train_dataloader)\n",
    "    t_X = t_X.to(DEVICE)\n",
    "    t_y = t_y.to(DEVICE)\n",
    "    output = lc.forward_train(t_X)\n",
    "    loss = loss_fn(output, t_y)\n",
    "    \n",
    "    if i in loss_steps:\n",
    "        print(f\"loss @ {i}th step: {loss} in {time.perf_counter() - start_time}s\")\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "    if i in viz_steps:\n",
    "        plt.scatter(X[class_bool, 0], X[class_bool, 1])\n",
    "        plt.scatter(X[np.logical_not(class_bool), 0], X[np.logical_not(class_bool), 1])\n",
    "        plt.plot(\n",
    "            np.arange(-3, 3, 0.1), \n",
    "            np.arange(-3, 3, 0.1)*lc.m.detach().cpu().numpy() + lc.b.detach().cpu().numpy(), \n",
    "            linewidth=2, color=\"red\"\n",
    "        )\n",
    "        plt.gca().set_title(f\"{i}th step\")\n",
    "        plt.show()\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc.to(\"cpu\")\n",
    "print(classification_report(\n",
    "    y,\n",
    "    lc.forward_test(X).numpy())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data\n",
    "plt.scatter(X[class_bool, 0], X[class_bool, 1])\n",
    "plt.scatter(X[np.logical_not(class_bool), 0], X[np.logical_not(class_bool), 1])\n",
    "plt.plot(\n",
    "    np.arange(-3, 3, 0.1), \n",
    "    np.arange(-3, 3, 0.1)*lc.m.detach().cpu().numpy() + lc.b.detach().cpu().numpy(), \n",
    "    linewidth=2, color=\"red\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 ('.venv-torch-metal': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a866963bc3cb7c7934c5ba68167a0b5deb814b0dc0feac21d008e455a3e431d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
