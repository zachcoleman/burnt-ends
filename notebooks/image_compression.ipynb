{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder-based Image Compression\n",
    "\n",
    "An example of using a simple AutoEncoder architecture to compress images. There are comparisons to JPEG encoding as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import io\n",
    "import functools\n",
    "import pathlib\n",
    "import glob\n",
    "from typing import Callable, List\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torchvision.io import ImageReadMode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cv2 import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = \"/Users/zachcoleman/burnt-ends/data/coco/val2017\"\n",
    "IMG_SIZE = (512, 512)\n",
    "DEVICE = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset:\n",
    "    def __init__(self, img_dir, transforms: List[Callable] = None, ext = \"jpg\"):\n",
    "        self.filepaths = glob.glob(str(pathlib.Path(img_dir) / f\"*.{ext}\"))\n",
    "        if transforms is None:\n",
    "            self.transforms = []\n",
    "        else:\n",
    "            self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = imread(self.filepaths[idx]).transpose([2, 0, 1])\n",
    "        if isinstance(img, np.ndarray):\n",
    "            img = torch.from_numpy(img)\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "    \n",
    "    @classmethod\n",
    "    def plot(cls, x, ax = None):\n",
    "        if ax is None:\n",
    "            return plt.imshow(x.permute([1, 2, 0]).numpy())\n",
    "        else:\n",
    "            return ax.imshow(x.permute([1, 2, 0]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, padding):\n",
    "        super().__init__()        \n",
    "        self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel, stride, padding)\n",
    "        self.norm = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.act = torch.nn.LeakyReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, compressed_output_channels):\n",
    "        super().__init__()        \n",
    "        self.compressed_output_channels = compressed_output_channels\n",
    "        self.multiply_expand_factor = torch.nn.Parameter(torch.tensor([100.]))\n",
    "        # self.encode_layers = [\n",
    "        #     ConvBlock(3, 32, 7, 2, 3),\n",
    "        #     ConvBlock(32, 64, 5, 2, 2),\n",
    "        #     ConvBlock(64, compressed_output_channels, 1, 1, \"same\")\n",
    "        # ]\n",
    "        # self.decode_layers = [\n",
    "        #     torch.nn.ConvTranspose2d(compressed_output_channels, 16, 4, 2, 1),\n",
    "        #     torch.nn.ConvTranspose2d(16, 32, 4, 2, 1),\n",
    "        #     ConvBlock(32, 3, 3, 1, \"same\")\n",
    "        # ]\n",
    "        self.encode_layers = [\n",
    "            ConvBlock(3, 16, 3, 2, 1),\n",
    "            ConvBlock(16, 32, 3, 2, 1),\n",
    "            ConvBlock(32, 64, 3, 1, \"same\"),\n",
    "            ConvBlock(64, 64, 3, 1, \"same\"),\n",
    "            ConvBlock(64, compressed_output_channels, 1, 1, \"same\")\n",
    "        ]\n",
    "        self.decode_layers = [\n",
    "            torch.nn.ConvTranspose2d(compressed_output_channels, 16, 4, 2, 1),\n",
    "            torch.nn.ConvTranspose2d(16, 32, 4, 2, 1),\n",
    "            ConvBlock(32, 16, 3, 1, \"same\"),\n",
    "            ConvBlock(16, 3, 1, 1, \"same\"),\n",
    "\n",
    "        ]\n",
    "        self.encode_layers = torch.nn.ModuleList(self.encode_layers)\n",
    "        self.decode_layers = torch.nn.ModuleList(self.decode_layers)\n",
    "        self.register_parameter(\"multiply_expand_factor\", self.multiply_expand_factor)\n",
    "        \n",
    "    def get_compression_ratio(self):\n",
    "        tmp = torch.randn(1, 3, 512, 512, device=next(self.parameters()).device)\n",
    "        compressed = self.compress(tmp)\n",
    "        data_pts = lambda l: functools.reduce(lambda x, y: x*y, l)\n",
    "        return  data_pts(compressed.shape) / data_pts(tmp.shape)\n",
    "    \n",
    "    def encode(self, x) -> torch.Tensor:\n",
    "        for l in self.encode_layers:\n",
    "            x = l(x)\n",
    "        x = x * self.multiply_expand_factor\n",
    "        x = self.limit_to_uint8(x)\n",
    "        return x\n",
    "    \n",
    "    def limit_to_uint8(self, x):\n",
    "        return torch.clamp(x, 0., 255.)\n",
    "\n",
    "    def normalize_uint8(self, x):\n",
    "        return (x-128.) / 128.\n",
    "    \n",
    "    def decode(self, x) -> torch.Tensor:\n",
    "        x = self.normalize_uint8(x)\n",
    "        for l in self.decode_layers:\n",
    "            x = l(x)\n",
    "        x = torch.clamp(x, 0., 1.)\n",
    "        return x\n",
    "\n",
    "    def compress(self, x) -> torch.Tensor:\n",
    "        x = self.encode(x)\n",
    "        x = torch.round(x)\n",
    "        return x\n",
    "\n",
    "    def decompress(self, x) -> torch.Tensor:\n",
    "        return self.decode(x)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.encode(inputs)\n",
    "        x = self.decode(x)\n",
    "        return x\n",
    "\n",
    "    def compress_decompress(self, inputs):\n",
    "        x = self.compress(inputs)\n",
    "        x = self.decompress(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_epochs = 1\n",
    "batch_size = 16\n",
    "down_time = 0\n",
    "num_sleeps = 2\n",
    "max_iter = 2_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = lambda x: torch.nn.functional.interpolate(x.unsqueeze(0), size=IMG_SIZE)[0]\n",
    "normalize = lambda x: x / 255.\n",
    "denormalize = lambda x: x * 255\n",
    "\n",
    "ds = ImageDataset(IMG_PATH, [resize, normalize])\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "auto_encoder = AutoEncoder(6)\n",
    "auto_encoder.to(DEVICE)\n",
    "# loss_fn = torch.nn.L1Loss()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optim = torch.optim.Adam(auto_encoder.parameters())\n",
    "\n",
    "# whole network training\n",
    "for epoch_num in range(num_of_epochs):\n",
    "    iter_data_loader = iter(dataloader)\n",
    "    start_time = time.perf_counter()\n",
    "    for n, X in enumerate(iter_data_loader):\n",
    "        if n + epoch_num*len(ds) > max_iter:\n",
    "            break\n",
    "        optim.zero_grad()\n",
    "        X = X.to(DEVICE)\n",
    "        output = auto_encoder(X)\n",
    "        loss = loss_fn(output, X)\n",
    "        if (n+1) % 100 == 0:\n",
    "            stop_time = time.perf_counter()\n",
    "            print(f\"epoch: {epoch_num+1} - step: {n+1}th - time: {stop_time-start_time:.2f}s - loss: {loss:.4f}\")\n",
    "            start_time = time.perf_counter()\n",
    "        time.sleep(down_time / num_sleeps)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        time.sleep(down_time / num_sleeps)\n",
    "\n",
    "# decode head only training\n",
    "for epoch_num in range(num_of_epochs):\n",
    "    iter_data_loader = iter(dataloader)\n",
    "    start_time = time.perf_counter()\n",
    "    for n, X in enumerate(iter_data_loader):\n",
    "        if n + epoch_num*len(ds) > max_iter:\n",
    "            break\n",
    "        optim.zero_grad()\n",
    "        X = X.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = auto_encoder.compress(X)\n",
    "        output = auto_encoder.decode(output)\n",
    "        loss = loss_fn(output, X)\n",
    "        if (n+1) % 100 == 0:\n",
    "            stop_time = time.perf_counter()\n",
    "            print(f\"epoch: {epoch_num+1} - step: {n+1}th - time: {stop_time-start_time:.2f}s - loss: {loss:.4f}\")\n",
    "            start_time = time.perf_counter()\n",
    "        time.sleep(down_time / num_sleeps)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        time.sleep(down_time / num_sleeps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.random.randint(0, len(ds))\n",
    "\n",
    "# visualize AutoEncoder compression\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 12))\n",
    "input_img = denormalize(ds[ix]).int()\n",
    "\n",
    "ds.plot(input_img, ax[0])\n",
    "with torch.inference_mode():\n",
    "    tmp = auto_encoder.forward(ds[ix].unsqueeze(0).to(DEVICE))[0].cpu()\n",
    "    tmp = denormalize(tmp).int()\n",
    "ds.plot(tmp, ax[1])\n",
    "ds.plot(torch.abs(tmp - input_img) / 255., ax[2])\n",
    "plt.show()\n",
    "\n",
    "# visualize JPEG compression\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 12))\n",
    "ds.plot(input_img, ax[0])\n",
    "\n",
    "# JPEG encode and decode\n",
    "img_bytes =  io.BytesIO()\n",
    "Image.fromarray(input_img.numpy().transpose([1, 2, 0]).astype(np.uint8)).save(img_bytes, format=\"JPEG\")\n",
    "jpg_img = Image.open(io.BytesIO(img_bytes.getvalue()))\n",
    "ax[1].imshow(jpg_img)\n",
    "\n",
    "jpg_tensor = torch.from_numpy(np.array(jpg_img).transpose(2, 0, 1))\n",
    "ds.plot(torch.abs(input_img - jpg_tensor) / 255., ax[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    np.quantile((torch.abs(tmp - input_img) / 255.).cpu().numpy(), \n",
    "    np.arange(0.01, 1, 0.01)),\n",
    "    label=\"DL\"\n",
    ")\n",
    "plt.plot(\n",
    "    np.quantile((torch.abs(input_img - jpg_tensor) / 255.).cpu().numpy(),\n",
    "    np.arange(0.01, 1, 0.01)),\n",
    "    label=\"JPEG\"\n",
    ")\n",
    "plt.gca().legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a866963bc3cb7c7934c5ba68167a0b5deb814b0dc0feac21d008e455a3e431d9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('.venv-torch-metal': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
